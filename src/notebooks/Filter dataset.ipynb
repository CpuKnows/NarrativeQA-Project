{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_index = pd.read_csv('../../data/documents.csv')\n",
    "questions = pd.read_csv('../../data/qaps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_index = docs_index.loc[lambda df: df['kind'] == 'gutenberg']\n",
    "questions = questions.loc[lambda df: df['question'].str.startswith('Who')]\n",
    "questions = questions.merge(docs_index.loc[:, ['document_id']], how='inner', on='document_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>set</th>\n",
       "      <th>kind</th>\n",
       "      <th>story_url</th>\n",
       "      <th>story_file_size</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>story_word_count</th>\n",
       "      <th>story_start</th>\n",
       "      <th>story_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/21572.txt.utf-8</td>\n",
       "      <td>814507</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Percival_Keene</td>\n",
       "      <td>Percival Keene</td>\n",
       "      <td>173334</td>\n",
       "      <td>Produced by Nick</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00936497f5884881f1df23f4834f6739552cee8b</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/3526.txt.utf-8</td>\n",
       "      <td>566874</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Five_Weeks_in_a_B...</td>\n",
       "      <td>Five Weeks in a Balloon</td>\n",
       "      <td>112898</td>\n",
       "      <td>Produced by Judy</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/42188.txt.utf-8</td>\n",
       "      <td>90192</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Shadows_in_the_Mo...</td>\n",
       "      <td>Shadows in the Moonlight (story)</td>\n",
       "      <td>17670</td>\n",
       "      <td>Produced by Greg</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00fb61fa7bee266ad995e52190ebb73606b60b70</td>\n",
       "      <td>valid</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/3771.txt.utf-8</td>\n",
       "      <td>372868</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Cynthia's_Revels</td>\n",
       "      <td>Cynthia's Revels</td>\n",
       "      <td>74928</td>\n",
       "      <td>Produced by Sue</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>014de1a8802c05ff64efa047e9290fb7fccea2b4</td>\n",
       "      <td>test</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/1329.txt.utf-8</td>\n",
       "      <td>560685</td>\n",
       "      <td>http://en.wikipedia.org/wiki/A_Voyage_to_Arcturus</td>\n",
       "      <td>A Voyage to Arcturus</td>\n",
       "      <td>113790</td>\n",
       "      <td>Produced by An</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                document_id    set       kind  \\\n",
       "0  0029bdbe75423337b551e42bb31f9a102785376f  train  gutenberg   \n",
       "1  00936497f5884881f1df23f4834f6739552cee8b  train  gutenberg   \n",
       "2  00950a3641e6a28b04a6fabf6334140e2deaa9fd  train  gutenberg   \n",
       "3  00fb61fa7bee266ad995e52190ebb73606b60b70  valid  gutenberg   \n",
       "4  014de1a8802c05ff64efa047e9290fb7fccea2b4   test  gutenberg   \n",
       "\n",
       "                                         story_url  story_file_size  \\\n",
       "0  http://www.gutenberg.org/ebooks/21572.txt.utf-8           814507   \n",
       "1   http://www.gutenberg.org/ebooks/3526.txt.utf-8           566874   \n",
       "2  http://www.gutenberg.org/ebooks/42188.txt.utf-8            90192   \n",
       "3   http://www.gutenberg.org/ebooks/3771.txt.utf-8           372868   \n",
       "4   http://www.gutenberg.org/ebooks/1329.txt.utf-8           560685   \n",
       "\n",
       "                                            wiki_url  \\\n",
       "0        http://en.wikipedia.org/wiki/Percival_Keene   \n",
       "1  http://en.wikipedia.org/wiki/Five_Weeks_in_a_B...   \n",
       "2  http://en.wikipedia.org/wiki/Shadows_in_the_Mo...   \n",
       "3      http://en.wikipedia.org/wiki/Cynthia's_Revels   \n",
       "4  http://en.wikipedia.org/wiki/A_Voyage_to_Arcturus   \n",
       "\n",
       "                         wiki_title  story_word_count       story_start  \\\n",
       "0                    Percival Keene            173334  Produced by Nick   \n",
       "1           Five Weeks in a Balloon            112898  Produced by Judy   \n",
       "2  Shadows in the Moonlight (story)             17670  Produced by Greg   \n",
       "3                  Cynthia's Revels             74928   Produced by Sue   \n",
       "4              A Voyage to Arcturus            113790    Produced by An   \n",
       "\n",
       "      story_end  \n",
       "0  new eBooks .  \n",
       "1  new eBooks .  \n",
       "2  new eBooks .  \n",
       "3  new eBooks .  \n",
       "4  new eBooks .  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>set</th>\n",
       "      <th>question</th>\n",
       "      <th>answer1</th>\n",
       "      <th>answer2</th>\n",
       "      <th>question_tokenized</th>\n",
       "      <th>answer1_tokenized</th>\n",
       "      <th>answer2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>Who is Miss Delmer?</td>\n",
       "      <td>the elderly spinster aunt of the Earl de Verse...</td>\n",
       "      <td>She's Captail Delmar's aunt.</td>\n",
       "      <td>Who is Miss Delmer ?</td>\n",
       "      <td>the elderly spinster aunt of the Earl de Verse...</td>\n",
       "      <td>She s Captail Delmar s aunt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>Who does Arabella Mason wed?</td>\n",
       "      <td>Ben Keene, Delmar's valet</td>\n",
       "      <td>Ben Keene</td>\n",
       "      <td>Who does Arabella Mason wed ?</td>\n",
       "      <td>Ben Keene , Delmar s valet</td>\n",
       "      <td>Ben Keene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>Who is the bully that steals Percival's lunch?</td>\n",
       "      <td>his teacher, Mr. O'Gallagher</td>\n",
       "      <td>The schoolmaster</td>\n",
       "      <td>Who is the bully that steals Percival s lunch ?</td>\n",
       "      <td>his teacher , Mr. O'Gallagher</td>\n",
       "      <td>The schoolmaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>Who does Percival convince the Pirates to spare?</td>\n",
       "      <td>a rich Dutch merchant and his daughter Minnie</td>\n",
       "      <td>A Dutch Merchant and his daughter</td>\n",
       "      <td>Who does Percival convince the Pirates to spare ?</td>\n",
       "      <td>a rich Dutch merchant and his daughter Minnie</td>\n",
       "      <td>A Dutch Merchant and his daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>Who lives at Madeline Hall?</td>\n",
       "      <td>Miss Delmar</td>\n",
       "      <td>Miss Delmar</td>\n",
       "      <td>Who lives at Madeline Hall ?</td>\n",
       "      <td>Miss Delmar</td>\n",
       "      <td>Miss Delmar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                document_id    set  \\\n",
       "0  0029bdbe75423337b551e42bb31f9a102785376f  train   \n",
       "1  0029bdbe75423337b551e42bb31f9a102785376f  train   \n",
       "2  0029bdbe75423337b551e42bb31f9a102785376f  train   \n",
       "3  0029bdbe75423337b551e42bb31f9a102785376f  train   \n",
       "4  0029bdbe75423337b551e42bb31f9a102785376f  train   \n",
       "\n",
       "                                           question  \\\n",
       "0                               Who is Miss Delmer?   \n",
       "1                      Who does Arabella Mason wed?   \n",
       "2    Who is the bully that steals Percival's lunch?   \n",
       "3  Who does Percival convince the Pirates to spare?   \n",
       "4                       Who lives at Madeline Hall?   \n",
       "\n",
       "                                             answer1  \\\n",
       "0  the elderly spinster aunt of the Earl de Verse...   \n",
       "1                          Ben Keene, Delmar's valet   \n",
       "2                       his teacher, Mr. O'Gallagher   \n",
       "3      a rich Dutch merchant and his daughter Minnie   \n",
       "4                                        Miss Delmar   \n",
       "\n",
       "                             answer2  \\\n",
       "0       She's Captail Delmar's aunt.   \n",
       "1                          Ben Keene   \n",
       "2                   The schoolmaster   \n",
       "3  A Dutch Merchant and his daughter   \n",
       "4                        Miss Delmar   \n",
       "\n",
       "                                  question_tokenized  \\\n",
       "0                               Who is Miss Delmer ?   \n",
       "1                      Who does Arabella Mason wed ?   \n",
       "2    Who is the bully that steals Percival s lunch ?   \n",
       "3  Who does Percival convince the Pirates to spare ?   \n",
       "4                       Who lives at Madeline Hall ?   \n",
       "\n",
       "                                   answer1_tokenized  \\\n",
       "0  the elderly spinster aunt of the Earl de Verse...   \n",
       "1                         Ben Keene , Delmar s valet   \n",
       "2                      his teacher , Mr. O'Gallagher   \n",
       "3      a rich Dutch merchant and his daughter Minnie   \n",
       "4                                        Miss Delmar   \n",
       "\n",
       "                   answer2_tokenized  \n",
       "0      She s Captail Delmar s aunt .  \n",
       "1                          Ben Keene  \n",
       "2                   The schoolmaster  \n",
       "3  A Dutch Merchant and his daughter  \n",
       "4                        Miss Delmar  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_index.to_csv('../../data/documents_books.csv', index=False)\n",
    "questions.to_csv('../../data/questions_books_who.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_index = pd.read_csv('../../data/documents_books.csv')\n",
    "questions = pd.read_csv('../../data/questions_books_who.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector entity similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ent_ws(ent):\n",
    "    return re.sub('[\\s]+', ' ', ent.text.strip())\n",
    "\n",
    "def get_people_ents_and_mentions(doc):\n",
    "    people_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Get person entities\n",
    "        if ent.label_ is 'PERSON':\n",
    "            people_ents.append(ent)\n",
    "            \n",
    "    ent_mentions = Counter([clean_ent_ws(ent) for ent in people_ents])\n",
    "    #ent_mentions = [k for k,v in ent_mentions.items() if v >= 5 and len(k) > 2]\n",
    "    return people_ents, ent_mentions\n",
    "\n",
    "def get_ent_most_similar(people_ents, question):\n",
    "    ent_similarity = {}\n",
    "    \n",
    "    for ent in people_ents:\n",
    "        similarity = ent.sent.similarity(question)\n",
    "\n",
    "        ent_str = clean_ent_ws(ent)\n",
    "        if (ent_str not in ent_similarity or \n",
    "                ent_similarity[ent_str]['similarity'] < similarity):\n",
    "            ent_similarity[ent_str] = {\n",
    "                'question': question.text,\n",
    "                'similarity': similarity,\n",
    "                'sentence': ent.sent.text,\n",
    "                'sent_start_char': ent.sent.start_char,\n",
    "                'sent_end_char': ent.sent.end_char\n",
    "            }\n",
    "            \n",
    "    return ent_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/5707 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_ent_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-94b8042f2cf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Final data structures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0ment_mentions_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ment_mentions_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mpeople_ents_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpeople_ents_total\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclean_ent_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ment_mentions_total\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcsv_output_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_doc_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-94b8042f2cf4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Final data structures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0ment_mentions_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ment_mentions_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mpeople_ents_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpeople_ents_total\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclean_ent_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ment_mentions_total\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcsv_output_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_doc_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_ent_str' is not defined"
     ]
    }
   ],
   "source": [
    "previous_doc_id = ''\n",
    "output_csv = ''\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "for i, row in tqdm(questions.iterrows(), total=len(questions)):\n",
    "    if row['document_id'] != previous_doc_id:\n",
    "        # process new doc\n",
    "        with codecs.open('../../data/clean/'+ row['document_id'] +'-clean.content', 'r', \n",
    "                         encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # Process doc in chunks of 500,000 characters\n",
    "        chunk_size = 500000\n",
    "        people_ents_total = []\n",
    "        ent_mentions_total = {}\n",
    "        chunk_num = 0\n",
    "        for start, end in zip(range(0, len(text), chunk_size), range(chunk_size, len(text)+chunk_size, chunk_size)):\n",
    "            doc = nlp(text[start:end])\n",
    "\n",
    "            # processing on doc chunk\n",
    "            people_ents, ent_mentions = get_people_ents_and_mentions(doc)\n",
    "\n",
    "            # combine people_ents and ent_mentions from other chunks\n",
    "            people_ents_total.extend(people_ents)\n",
    "            for k,v in ent_mentions.items():\n",
    "                if k not in ent_mentions_total:\n",
    "                    ent_mentions_total.update({k: v})\n",
    "                else:\n",
    "                    ent_mentions_total[k] = ent_mentions_total[k] + v\n",
    "\n",
    "        # Final data structures\n",
    "        ent_mentions_total = [k for k,v in ent_mentions_total.items() if v >= 5 and len(k) > 2]\n",
    "        people_ents_total = [ent for ent in people_ents_total if clean_ent_ws(ent) in ent_mentions_total]\n",
    "        \n",
    "        csv_output_id = previous_doc_id\n",
    "        previous_doc_id = row['document_id']\n",
    "        \n",
    "    else:\n",
    "        # use already processed doc\n",
    "        pass\n",
    "    \n",
    "    # Question-sentence similarity\n",
    "    question = nlp(row['question'])\n",
    "    ent_similarity = get_ent_most_similar(people_ents_total, question)\n",
    "    \n",
    "    if csv_output_id != '' and len(output_df) != 0:\n",
    "        # If we reach the end of a set of doc questions then output csv\n",
    "        output_df.to_csv('../../data/ent_candidates/'+ csv_output_id +'-candidates.csv', index=False)\n",
    "        output_df = pd.DataFrame()\n",
    "        csv_output_id = ''\n",
    "    elif i == len(questions) - 1:\n",
    "        # Output csv of the last iteration\n",
    "        df = pd.DataFrame.from_dict(ent_similarity, orient='index')\n",
    "        df.reset_index(level=0, inplace=True)\n",
    "        df.rename(columns={'index':'entity'}, inplace=True)\n",
    "        df['document_id'] = row['document_id']\n",
    "        output_df = pd.concat([output_df, df])\n",
    "    \n",
    "        output_df.to_csv('../../data/ent_candidates/'+ row['document_id'] +'-candidates.csv', index=False)\n",
    "        output_df = pd.DataFrame()\n",
    "        csv_output_id = ''\n",
    "        \n",
    "    # Append question record to datafram for this document\n",
    "    df = pd.DataFrame.from_dict(ent_similarity, orient='index')\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.rename(columns={'index':'entity'}, inplace=True)\n",
    "    df['document_id'] = row['document_id']\n",
    "    output_df = pd.concat([output_df, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_synset_id(sentence):\n",
    "    syn_list = []\n",
    "    for token in sentence:\n",
    "        wn_tag = None\n",
    "        if token.pos_.startswith('N'):\n",
    "            wn_tag = 'n'\n",
    "        if token.pos_.startswith('V'):\n",
    "            wn_tag = 'v'\n",
    "        if token.pos_.startswith('J'):\n",
    "            wn_tag = 'a'\n",
    "        if token.pos_.startswith('R'):\n",
    "            wn_tag = 'r'\n",
    "        \n",
    "        if wn_tag is not None:\n",
    "            try:\n",
    "                syn_id = wn.synsets(token.lemma_, wn_tag)[0]\n",
    "                if syn_id is not None:\n",
    "                    syn_list.append(syn_id)\n",
    "            except:\n",
    "                syn_id = None\n",
    "    \n",
    "    return syn_list\n",
    "\n",
    "def synset_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Get synsets\n",
    "    syn_list1 = token_to_synset_id(sentence1)\n",
    "    syn_list2 = token_to_synset_id(sentence2)\n",
    "    \n",
    "    score, count = 0.0, 0\n",
    "    # For each word in the first sentence\n",
    "    for synset in syn_list1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = None\n",
    "        sim_list = [synset.path_similarity(ss) or 0 for ss in syn_list2]\n",
    "        if len(sim_list) != 0:\n",
    "            best_score = max(sim_list)\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "\n",
    "    # Average the values\n",
    "    if count != 0:\n",
    "        score /= count\n",
    "        return score\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2689123376623377"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = nlp('Shortly afterwards, Captain Delmar again came over to Madeline Hall, accompanied as usual, by Ben, and ' + \n",
    "    'the second day after their arrival it was made known to all whom it might concern, that Miss Arabella Mason' +\n",
    "    ' had actually contracted a secret marriage with the handsome Benjamin Keene.')\n",
    "\n",
    "sent2 = nlp('Who does Arabella Mason wed?')\n",
    "\n",
    "synset_similarity(sent1, sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.71464431675264\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r1 -n1 -c\n",
    "\n",
    "sim_list = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sim_list.append(sent.similarity(sent2))\n",
    "    \n",
    "    last_sent = sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('write.v.01').path_similarity(wn.synset('write.v.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.edit_distance('Amelia', 'Arabella')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrativeqa]",
   "language": "python",
   "name": "conda-env-narrativeqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
