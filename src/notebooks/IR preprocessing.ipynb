{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=[\"parser\", \"ner\"], max_length=10**7) #\"tagger\",  \n",
    "#nlp = spacy.load('en', disable=[\"ner\"])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    1102\n",
       "test      355\n",
       "valid     115\n",
       "Name: set, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#docs_index.loc[lambda df: df['kind'] == 'gutenberg', 'set'].value_counts()\n",
    "docs_index['set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>set</th>\n",
       "      <th>kind</th>\n",
       "      <th>story_url</th>\n",
       "      <th>story_file_size</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>wiki_title</th>\n",
       "      <th>story_word_count</th>\n",
       "      <th>story_start</th>\n",
       "      <th>story_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>movie</td>\n",
       "      <td>http://www.awesomefilm.com/script/pumpupthevol...</td>\n",
       "      <td>54078</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Pump_Up_the_Volum...</td>\n",
       "      <td>Pump Up the Volume (film)</td>\n",
       "      <td>11499</td>\n",
       "      <td>Happy Harry Hardon</td>\n",
       "      <td>by Martin Eaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0029bdbe75423337b551e42bb31f9a102785376f</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/21572.txt.utf-8</td>\n",
       "      <td>814507</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Percival_Keene</td>\n",
       "      <td>Percival Keene</td>\n",
       "      <td>173334</td>\n",
       "      <td>Produced by Nick</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00936497f5884881f1df23f4834f6739552cee8b</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/3526.txt.utf-8</td>\n",
       "      <td>566874</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Five_Weeks_in_a_B...</td>\n",
       "      <td>Five Weeks in a Balloon</td>\n",
       "      <td>112898</td>\n",
       "      <td>Produced by Judy</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00950a3641e6a28b04a6fabf6334140e2deaa9fd</td>\n",
       "      <td>train</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>http://www.gutenberg.org/ebooks/42188.txt.utf-8</td>\n",
       "      <td>90192</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Shadows_in_the_Mo...</td>\n",
       "      <td>Shadows in the Moonlight (story)</td>\n",
       "      <td>17670</td>\n",
       "      <td>Produced by Greg</td>\n",
       "      <td>new eBooks .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00ee9e01a0e581e0d8cbf7e865a895147c480c5e</td>\n",
       "      <td>train</td>\n",
       "      <td>movie</td>\n",
       "      <td>http://www.imsdb.com/scripts/Crank.html</td>\n",
       "      <td>309143</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crank_(film)</td>\n",
       "      <td>Crank (film)</td>\n",
       "      <td>27546</td>\n",
       "      <td>CRANK Written by</td>\n",
       "      <td>TO SOUNDTRACK .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                document_id    set       kind  \\\n",
       "0  0025577043f5090cd603c6aea60f26e236195594   test      movie   \n",
       "1  0029bdbe75423337b551e42bb31f9a102785376f  train  gutenberg   \n",
       "2  00936497f5884881f1df23f4834f6739552cee8b  train  gutenberg   \n",
       "3  00950a3641e6a28b04a6fabf6334140e2deaa9fd  train  gutenberg   \n",
       "4  00ee9e01a0e581e0d8cbf7e865a895147c480c5e  train      movie   \n",
       "\n",
       "                                           story_url  story_file_size  \\\n",
       "0  http://www.awesomefilm.com/script/pumpupthevol...            54078   \n",
       "1    http://www.gutenberg.org/ebooks/21572.txt.utf-8           814507   \n",
       "2     http://www.gutenberg.org/ebooks/3526.txt.utf-8           566874   \n",
       "3    http://www.gutenberg.org/ebooks/42188.txt.utf-8            90192   \n",
       "4            http://www.imsdb.com/scripts/Crank.html           309143   \n",
       "\n",
       "                                            wiki_url  \\\n",
       "0  http://en.wikipedia.org/wiki/Pump_Up_the_Volum...   \n",
       "1        http://en.wikipedia.org/wiki/Percival_Keene   \n",
       "2  http://en.wikipedia.org/wiki/Five_Weeks_in_a_B...   \n",
       "3  http://en.wikipedia.org/wiki/Shadows_in_the_Mo...   \n",
       "4          http://en.wikipedia.org/wiki/Crank_(film)   \n",
       "\n",
       "                         wiki_title  story_word_count         story_start  \\\n",
       "0         Pump Up the Volume (film)             11499  Happy Harry Hardon   \n",
       "1                    Percival Keene            173334    Produced by Nick   \n",
       "2           Five Weeks in a Balloon            112898    Produced by Judy   \n",
       "3  Shadows in the Moonlight (story)             17670    Produced by Greg   \n",
       "4                      Crank (film)             27546    CRANK Written by   \n",
       "\n",
       "         story_end  \n",
       "0  by Martin Eaves  \n",
       "1     new eBooks .  \n",
       "2     new eBooks .  \n",
       "3     new eBooks .  \n",
       "4  TO SOUNDTRACK .  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_index = pd.read_csv('../../data/documents.csv')\n",
    "docs_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>set</th>\n",
       "      <th>question</th>\n",
       "      <th>answer1</th>\n",
       "      <th>answer2</th>\n",
       "      <th>question_tokenized</th>\n",
       "      <th>answer1_tokenized</th>\n",
       "      <th>answer2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>Who is Mark Hunter?</td>\n",
       "      <td>He is a high school student in Phoenix.</td>\n",
       "      <td>A loner and outsider student with a radio stat...</td>\n",
       "      <td>Who is Mark Hunter ?</td>\n",
       "      <td>He is a high school student in Phoenix .</td>\n",
       "      <td>A loner and outsider student with a radio stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>Where does this radio station take place?</td>\n",
       "      <td>It takes place in Mark's parents basement.</td>\n",
       "      <td>Phoenix, Arizona</td>\n",
       "      <td>Where does this radio station take place ?</td>\n",
       "      <td>It takes place in Mark s parents basement .</td>\n",
       "      <td>Phoenix , Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>Why do more students tune into Mark's show?</td>\n",
       "      <td>Mark talks about what goes on at school and in...</td>\n",
       "      <td>Because he has a thing to say about what is ha...</td>\n",
       "      <td>Why do more students tune into Mark s show ?</td>\n",
       "      <td>Mark talks about what goes on at school and in...</td>\n",
       "      <td>Because he has a thing to say about what is ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>Who commits suicide?</td>\n",
       "      <td>Malcolm.</td>\n",
       "      <td>Malcolm.</td>\n",
       "      <td>Who commits suicide ?</td>\n",
       "      <td>Malcolm .</td>\n",
       "      <td>Malcolm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0025577043f5090cd603c6aea60f26e236195594</td>\n",
       "      <td>test</td>\n",
       "      <td>What does Paige jam into her microwave?</td>\n",
       "      <td>She jams her medals and accolades.</td>\n",
       "      <td>Her award medals</td>\n",
       "      <td>What does Paige jam into her microwave ?</td>\n",
       "      <td>She jams her medals and accolades .</td>\n",
       "      <td>Her award medals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                document_id   set  \\\n",
       "0  0025577043f5090cd603c6aea60f26e236195594  test   \n",
       "1  0025577043f5090cd603c6aea60f26e236195594  test   \n",
       "2  0025577043f5090cd603c6aea60f26e236195594  test   \n",
       "3  0025577043f5090cd603c6aea60f26e236195594  test   \n",
       "4  0025577043f5090cd603c6aea60f26e236195594  test   \n",
       "\n",
       "                                      question  \\\n",
       "0                          Who is Mark Hunter?   \n",
       "1    Where does this radio station take place?   \n",
       "2  Why do more students tune into Mark's show?   \n",
       "3                         Who commits suicide?   \n",
       "4      What does Paige jam into her microwave?   \n",
       "\n",
       "                                             answer1  \\\n",
       "0            He is a high school student in Phoenix.   \n",
       "1        It takes place in Mark's parents basement.    \n",
       "2  Mark talks about what goes on at school and in...   \n",
       "3                                           Malcolm.   \n",
       "4                She jams her medals and accolades.    \n",
       "\n",
       "                                             answer2  \\\n",
       "0  A loner and outsider student with a radio stat...   \n",
       "1                                   Phoenix, Arizona   \n",
       "2  Because he has a thing to say about what is ha...   \n",
       "3                                           Malcolm.   \n",
       "4                                   Her award medals   \n",
       "\n",
       "                             question_tokenized  \\\n",
       "0                          Who is Mark Hunter ?   \n",
       "1    Where does this radio station take place ?   \n",
       "2  Why do more students tune into Mark s show ?   \n",
       "3                         Who commits suicide ?   \n",
       "4      What does Paige jam into her microwave ?   \n",
       "\n",
       "                                   answer1_tokenized  \\\n",
       "0           He is a high school student in Phoenix .   \n",
       "1        It takes place in Mark s parents basement .   \n",
       "2  Mark talks about what goes on at school and in...   \n",
       "3                                          Malcolm .   \n",
       "4                She jams her medals and accolades .   \n",
       "\n",
       "                                   answer2_tokenized  \n",
       "0  A loner and outsider student with a radio stat...  \n",
       "1                                  Phoenix , Arizona  \n",
       "2  Because he has a thing to say about what is ha...  \n",
       "3                                          Malcolm .  \n",
       "4                                   Her award medals  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.read_csv('../../data/qaps.csv')\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11389\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,row in questions.iterrows():\n",
    "    if row['question'][:3].lower() == 'who':\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "first_words = []\n",
    "for i,row in questions.iterrows():\n",
    "    first_words.append(row['question_tokenized'].split()[0].lower())\n",
    "\n",
    "counts = Counter(first_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "argparse.ArgumentParser().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 17928),\n",
       " ('who', 11033),\n",
       " ('how', 4888),\n",
       " ('why', 4568),\n",
       " ('where', 3514),\n",
       " ('which', 1015),\n",
       " ('when', 768),\n",
       " ('in', 548),\n",
       " ('after', 281),\n",
       " ('whose', 260)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_doc(doc, chunk_size):\n",
    "    chunks = list()\n",
    "    chunk = ''\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        chunk += token.text_with_ws\n",
    "        if (i+1) % chunk_size == 0:\n",
    "            chunks.append(chunk)\n",
    "            chunk = ''\n",
    "\n",
    "    if chunk != '':\n",
    "        chunks.append(chunk)\n",
    "        chunk = ''\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def doc_ir(chunks, question, vectorizer, top_n):\n",
    "    chunks.append(question)\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
    "    tfidf_matrix = tfidf_matrix.todense()\n",
    "    \n",
    "    question_vector = tfidf_matrix[-1, :]\n",
    "    chunk_vectors = tfidf_matrix[0:-1, :]\n",
    "    \n",
    "    similarity_rank = list()\n",
    "    for i,vector in enumerate(chunk_vectors):\n",
    "        cos_sim = 1 - cosine(question_vector, vector)\n",
    "        similarity_rank.append((i, cos_sim))\n",
    "\n",
    "    similarity_rank.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    similarity_rank = similarity_rank[:top_n]\n",
    "    similarity_rank.sort(key=lambda tup: tup[0])\n",
    "    \n",
    "    return [chunks[i] for i,similarity in similarity_rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('../../data/ir_chunk_dataset2.csv', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    prev_doc_id = ''\n",
    "    for index, row in tqdm(questions.iterrows(), total=len(questions)):\n",
    "        doc_id, q, a = row['document_id'], row['question'], row['answer1']\n",
    "        if prev_doc_id != doc_id:\n",
    "            with codecs.open('../../data/clean/'+doc_id+'-clean.content', 'r', encoding='utf-8', errors='ignore') as g:\n",
    "                doc = nlp(g.read())\n",
    "            prev_doc_id = doc_id\n",
    "            chunks = chunk_doc(doc, 20)\n",
    "            #chunks = [sent.text for sent in doc.sents]\n",
    "\n",
    "        ir_chunks = doc_ir(chunks, q, tfidf_vectorizer, 5)\n",
    "        ir_output = '<del>'.join(ir_chunks)\n",
    "        ir_output += '</c>'\n",
    "        f.write(\"{}\\n\".format(ir_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"'\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "with codecs.open('../../data/ir_chunk_dataset.csv', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    f.write(\"'document_id','text','question','answer'\\n\")\n",
    "    \n",
    "    prev_doc_id = ''\n",
    "    for index, row in tqdm(questions.iterrows(), total=len(questions)):\n",
    "        doc_id, q, a = row['document_id'], row['question'], row['answer1']\n",
    "        if prev_doc_id != doc_id:\n",
    "            with codecs.open('../../data/anonymized_entities/'+doc_id+'-clean.content', 'r', encoding='utf-8', errors='ignore') as g:\n",
    "                doc = nlp(g.read())\n",
    "            prev_doc_id = doc_id\n",
    "            chunks = chunk_doc(doc, 20)\n",
    "            #chunks = [sent.text for sent in doc.sents]\n",
    "\n",
    "        ir_chunks = doc_ir(chunks, q, tfidf_vectorizer, 5)\n",
    "        ir_output = '<del>'.join(ir_chunks)\n",
    "        ir_output += '</c>'\n",
    "\n",
    "        ir_output = regex.sub(\"\\'\", ir_output)\n",
    "        q = regex.sub(\"\\'\", q)\n",
    "        a = regex.sub(\"\\'\", a)\n",
    "        f.write(\"'{}','{}','{}','{}'\\n\".format(doc_id, ir_chunks, q, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('../../data/ir_chunk_dataset2.csv', 'w', encoding='utf-8', errors='ignore') as f:\n",
    "    prev_doc_id = ''\n",
    "    \n",
    "    for index, row in tqdm(questions.iterrows(), total=len(questions)):\n",
    "        doc_id, q, a = row['document_id'], row['question'], row['answer1']\n",
    "        doc_q = nlp(q)\n",
    "        \n",
    "        if prev_doc_id != doc_id:\n",
    "            with codecs.open('../../data/clean/'+doc_id+'-clean.content', 'r', encoding='utf-8', errors='ignore') as g:\n",
    "                doc = nlp(g.read())\n",
    "            prev_doc_id = doc_id\n",
    "\n",
    "        sent_rank = list()    \n",
    "        for i,sent in enumerate(doc.sents):\n",
    "            sent_rank.append((i, sent.similarity(doc_q), sent.text))\n",
    "            \n",
    "        sent_rank.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        sent_rank = sent_rank[:5]\n",
    "        sent_rank.sort(key=lambda tup: tup[0])\n",
    "\n",
    "        ir_chunks = [sent for i,similarity,sent in sent_rank]\n",
    "        ir_output = '<del>'.join(ir_chunks)\n",
    "        ir_output += '</c>'\n",
    "        f.write(\"{}\\n\".format(ir_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('../../data/anonymized_entities/0025577043f5090cd603c6aea60f26e236195594-clean.content', 'r',encoding='utf-8', errors='ignore') as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.append(questions['question'].tolist()[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 1493)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(chunks)\n",
    "tfidf_matrix = tfidf_matrix.todense()\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 0.05963392467389739),\n",
       " (14, 0.04539162697097321),\n",
       " (33, 0.03883041978135293),\n",
       " (38, 0.09886007582915535),\n",
       " (53, 0.03920798878490961)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "question_vector = tfidf_matrix[-1, :]\n",
    "chunk_vectors = tfidf_matrix[0:-1, :]\n",
    "\n",
    "similarity_rank = list()\n",
    "for i,vector in enumerate(chunk_vectors):\n",
    "    cos_sim = 1 - cosine(question_vector, vector)\n",
    "    similarity_rank.append((i, cos_sim))\n",
    "        \n",
    "similarity_rank.sort(key=lambda tup: tup[1], reverse=True)\n",
    "similarity_rank = similarity_rank[:5]\n",
    "similarity_rank.sort(key=lambda tup: tup[0])\n",
    "similarity_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_chunks = [chunks[i] for i,similarity in similarity_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does Paige jam into her microwave?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions['question'].tolist()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me a bit about what you do.\n",
      "\n",
      "@entity72 - @entity116 run a comprehensive @entity158 values program, erm in which we discuss \n",
      "ethical situations, sex education and drug abuse.\n",
      "\n",
      "Happy @entity158 @entity128 - What do you say to young people who look around at the world \n",
      "and see it's become, like you know, a sleazy country, a place you just can't trust. Like \n",
      "your school for example. Why is it, it wins all of these awards and students are dropping \n",
      "out like flies, why..why is that. Now my listeners are interested in the decision to expel \n",
      "@entity145 @entity69.\n",
      "\n",
      "@entity72 - @entity116, erm, @entity116'm not aware of anything like that, @entity116 don't know what you're talking \n",
      "about.\n",
      "\n",
      "Happy @entity158 @entity128 - That is not true sir. \"@entity145 refuses to accept suggestions of a \n",
      "more positive mental attitude towards her health and her future. @entity116'm afraid @entity116 find no \n",
      "alternative, but to suggest suspension.\"\n",
      "\n",
      "@entity72 - Who is this? How \n"
     ]
    }
   ],
   "source": [
    "print(chunks[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrixlib.defmatrix.matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_matrix[-1, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrativeqa]",
   "language": "python",
   "name": "conda-env-narrativeqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
